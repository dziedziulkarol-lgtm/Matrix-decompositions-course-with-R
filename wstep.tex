\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{listingsutf8}

% konfiguracja dla R
\lstset{
  inputencoding=utf8,
  language=R,
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{gray!10},
  frame=single,
  breaklines=true,
  keywordstyle=\color{blue},
  commentstyle=\color{teal},
  stringstyle=\color{brown},
  showstringspaces=false,
  literate=
    {ą}{{\k{a}}}1 {Ą}{{\k{A}}}1
    {ć}{{\'c}}1 {Ć}{{\'C}}1
    {ę}{{\k{e}}}1 {Ę}{{\k{E}}}1
    {ł}{{\l{}}}1 {Ł}{{\L{}}}1
    {ń}{{\'n}}1 {Ń}{{\'N}}1
    {ó}{{\'o}}1 {Ó}{{\'O}}1
    {ś}{{\'s}}1 {Ś}{{\'S}}1
    {ż}{{\.z}}1 {Ż}{{\.Z}}1
    {ź}{{\'z}}1 {Ź}{{\'Z}}1
    {–}{{-}}1
}

\title{Lekcja 6: Od problemów praktycznych do SVD}
\author{}
\date{}

\begin{document}
\maketitle

\begin{quote}
\emph{„Każda macierz ma w sobie ukrytą strukturę. Pseudoodwrotność i przybliżenia rangowe
to pierwsze kroki do jej odkrycia.”}
\end{quote}

\section*{Cel lekcji}
\begin{itemize}
  \item Uporządkować wiedzę z lekcji 5 (sprzeczne układy, pseudoodwrotność, kompresja).
  \item Zobaczyć wspólny wzorzec: \textbf{rzuty}, \textbf{najlepsze przybliżenia}, \textbf{ortogonalność}.
  \item Przygotować intuicję do rozkładu SVD, który formalnie omówimy w kolejnych zajęciach.
\end{itemize}

\section{Sprzeczne układy i pseudoodwrotność}
\textbf{Przypomnienie:} układ
\[
x+y=1,\quad x+y=2,\quad x-y=0
\]
nie ma rozwiązania dokładnego.

\begin{enumerate}[label=\textbf{Z\arabic*}.]
  \item Wyznacz przybliżone rozwiązanie LSQ metodą pseudoodwrotności.
  \item Sprawdź, że $Ax^\star$ jest \emph{rzutem} $b$ na kolumny macierzy $A$.
\end{enumerate}

\begin{lstlisting}
A <- matrix(c(1,1,
              1,1,
              1,-1), nrow=3, byrow=TRUE)
b <- c(1,2,0)

sv <- svd(A)
Sigma_plus <- diag(1/sv$d, nrow=length(sv$d))
A_pinv <- sv$v %*% Sigma_plus %*% t(sv$u)
x_hat <- A_pinv %*% b
x_hat
\end{lstlisting}

\section{Kompresja macierzy i obrazów}
\begin{itemize}
  \item Aproksymacja rangi $k$ to szukanie macierzy prostszej, ale jak najbliższej oryginałowi.
  \item W praktyce: mniej danych, zachowanie głównych cech obrazu.
\end{itemize}

\begin{enumerate}[resume,label=\textbf{Z\arabic*}.]
  \item Policz przybliżenie rangi $1,2,3$ dla macierzy $8\times 8$ (np. szachownicy).
  \item Zaobserwuj, jak zmienia się jakość przybliżenia i błąd $\|A-A_k\|_F$.
\end{enumerate}

\begin{lstlisting}
img <- outer(1:8, 1:8, function(i,j) ifelse((i+j)%%2==0,1,0))
sv_img <- svd(img)

rank_k_approx <- function(svd_obj, k) {
  U <- svd_obj$u[,1:k]
  D <- diag(svd_obj$d[1:k])
  V <- svd_obj$v[,1:k]
  U %*% D %*% t(V)
}

for (k in 1:3) {
  Ak <- rank_k_approx(sv_img, k)
  err <- norm(img - Ak, "F")
  print(err)
}
\end{lstlisting}

\section{Most do SVD}
\begin{itemize}
  \item Widzimy trzy elementy wspólne:
  \begin{enumerate}
    \item Rzuty ortogonalne.
    \item Najlepsze przybliżenia.
    \item Struktura ortogonalna.
  \end{enumerate}
  \item Wszystko to formalnie łączy \textbf{Singular Value Decomposition}.
\end{itemize}

\section*{English Corner}
\begin{quote}
\textbf{Key idea:} Every matrix can be decomposed into three simple parts: 
an orthogonal transformation, a stretching along coordinate axes, 
and another orthogonal transformation.

\medskip
\textbf{Your task:}
\begin{enumerate}
  \item Translate the sentence above into Polish (without using a dictionary).
  \item Check with AI if your translation is correct.
  \item Advanced: Write one more sentence in English that explains 
        what an orthogonal projection is.
\end{enumerate}
\end{quote}


\section*{Pytanie do grupy}
\begin{quote}
„Dlaczego w analizie danych często zgadzamy się na przybliżenie, zamiast wymagać dokładności?”
\end{quote}

\end{document}
